---
title: "Predictive performance of multi-model ensemble forecasts of COVID-19 across European nations"
output:
  workflowr::wflow_html:
    toc: true
    toc_depth: 3
bibliography: references.bib
link-citations: true
csl: ieee.csl
editor_options:
  chunk_output_type: console
---
```{r, include=FALSE}
gdrive_path <- "trackdown/euro-hub-ensemble"
this_doc <- "analysis/euro-hub-ensemble-draft.Rmd"
#--------------------------------------------------------------------
# HOW TO EDIT THIS DOCUMENT
# 1. Before editing, run this chunk and then get latest from google docs with:
# trackdown::download_file(file = this_doc, gpath = gdrive_path)
# 2. When done editing, push changes to google docs 
# trackdown::update_file(file = this_doc, gpath = gdrive_path, hide_code = TRUE)
# 3. Build doc and push to website
# workflowr::wflow_publish(all = TRUE, update = TRUE)
# 4. Optionally, build PDF: 
# rmarkdown::render(this_doc, rmarkdown::pdf_document())
# 
# View draft: 
# [Rendered](https://covid19-forecast-hub-europe.github.io/euro-hub-ensemb# le/euro-hub-ensemble-draft.html)
# [Google doc](https://docs.google.com/document/d/1XT7xwljCfaQ1_CpKxyJySqiPsIZ2bWNWFJE_v_U6EHc/edit)
#--------------------------------------------------------------------
```


```{r set up, include=FALSE}
# Packages
library(here)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(ggplot2)
library(forcats)
library(patchwork)
library(gghighlight)
# Settings
here::i_am(this_doc)
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, 
                      message = FALSE, warning = FALSE,
                      eval.after = "fig.cap")
theme_set(theme_bw())
```

_Order tbc;_ Katharine Sherratt, Hugo Gruson, _Any co-authors_, _Team authors_, _Advisory team authors_, _ECDC authors_, Johannes Bracher, Sebastian Funk

# Abstract

_Background_ Short-term forecasts of infectious disease burden can contribute to situational awareness and aid capacity planning. Based on best practice in other fields and recent insights in infectious disease epidemiology, one can maximise the predictive performance of such forecasts if multiple models are combined into an ensemble. Here we report on the performance of ensembles created from over 40 models in predicting COVID-19 cases and deaths across Europe.

_Methods_ We adopted existing infrastructure to develop a public European COVID-19 Forecast Hub. We invited groups globally to contribute weekly forecasts for COVID-19 cases and deaths over the next one to four weeks. Forecasts included quantiles across the predictive distribution. Each week we created an ensemble forecast, where each predictive quantile was calculated as the equally-weighted average of all individual models' predictive quantiles. We retrospectively explored alternative methods for ensemble forecasts, including weighted averages based on models' past predictive skill. The performance of the ensembles was compared to individual models and a baseline model of no change using pairwise comparison with the Weighted Interval Score (WIS).

_Results_ We collected and combined 36 weeks of forecasts for 32 countries from 43 forecast models. We found the weekly ensemble had among the most reliable performances across countries over time, outperforming the baseline for 67% and 91% of targets for cases and deaths, respectively. Ensemble performance declined with increasing forecast time horizon when forecasting cases but remained stable for 4 weeks for incident death forecasts. Among several choices of ensemble methods, we found that regardless of methods for weighting component forecasts, the calculation of the average was the most influential choice for performance, with almost any forecast created using a median average performing better than using a mean.

_Conclusions_ Our results support the use of an ensemble as a reliable way to make real-time forecasts across many populations and epidemiological targets during infectious disease epidemics. We recommend the use of median ensemble methods, and that policy relevant work that uses ensembles should place more confidence in forecasts of incident death than case counts, particularly at longer (more than 2 week) periods into the future.

# Background

Epidemiological forecasts make quantitative statements about a disease outcome in the near future. At the most general level, forecasting targets can include measures of prevalent or incident disease and its severity, for some population over a specified time horizon. Researchers, policy makers, and the general public have used such forecasts to understand and respond to the global outbreaks of COVID-19 since early 2020 [@basshuysenThreeWaysWhich2021]. However forecasters use a variety of methods and models for creating and publishing forecasts, varying in both defining the forecast outcome and in reporting the distribution of probability around outcomes [@zelnerAccountingUncertaintyPandemic2021; @jamesUseMisuseMathematical2021]. Such variation between forecasts makes it difficult to interpret the likelihood of any one outcome, or to compare the predictive performance between forecast models. These barriers to comparing and evaluating mean there is little objective support for using any one particular forecast for representing or acting on likely outcomes.

A "forecast hub" is a centralised effort to improve the transparency and usefulness of forecasts, by standardising and collating the work of many independent teams producing forecasts [@reichCollaborativeMultiyearMultimodel2019]. A hub sets a commonly agreed structure for forecast targets, such as type of disease event, spatio-temporal units, or the set of quantiles of the probability distribution to include from probabilistic forecasts. Forecasters can adopt this format and contribute forecasts for centralised storage in the public domain. This shared infrastructure allows forecasts produced from diverse teams and methods to be visualised and quantitatively compared on a near-exact like-for-like basis, which can strengthen public and policy use of disease forecasts [@cdcCoronavirusDisease20192020]. The underlying approach to creating a forecast hub was pioneered for forecasting influenza in the USA and adapted for forecasts of short-term COVID-19 cases and deaths in the US [@rayEnsembleForecastsCoronavirus2020e], with similar efforts elsewhere [@bracherPreregisteredShorttermForecasting2021; @funkShorttermForecastsInform2020; @bicherSupportingCOVID19PolicyMaking2021].

Standardising forecasts may also allow us to maximise their predictive performance by combining multiple forecasts into a single ensemble forecast. Evidence from previous efforts in multi-model infectious disease forecasting  suggests that forecasts from an ensemble of many forecast models can be consistently high performing compared to any one of the component models [@reichAccuracyRealtimeMultimodel2019a; @johanssonOpenChallengeAdvance2019; @viboudRAPIDDEbolaForecasting2018]. Somewhat comparably, weather forecasting has a long standing use of building ensembles of many models using diverse methods with standardised data and formatting [@buizzaIntroductionSpecialIssue2019; @moranEpidemicForecastingMessier2016a].

The European COVID-19 Forecast Hub [@europeancovid-19forecasthubEuropeanCOVID19Forecast2021] is a project to collate short term forecasts of COVID-19 across 32 countries in the European region. The Hub is supported by the European Centre for Disease Control (ECDC), with the primary aim to provide reliable information about the near-term epidemiology of the COVID-19 pandemic to the research and policy communities and the general public. Second, the hub aims to create infrastructure for storing and analysing epidemiological forecasts made in real time by diverse research teams and methods across Europe. Third, the hub aims to maintain a community of infectious disease modellers underpinned by open science principles. We started formally collating and combining contributions to the European Forecast Hub in March 2021. Here, we investigate the predictive performance of an ensemble of all forecasts contributed to the hub in real time each week, as well as the performance of variations of ensemble methods created retrospectively.

# Methods

We developed infrastructure to host and analyse forecasts, following a similar structure and data format, and adapted processes and software provided by the US [@cramerReichlabCovid19forecasthubRelease2021; @wangReichlabCovidHubUtilsRepository2021] and the German and Polish COVID-19 [@bracherGermanPolishCOVID192020] forecast hubs. 

### Forecast targets and standardisation 

We sought forecasts for reported weekly incident counts of cases and deaths from COVID-19 for each of 32 countries in Europe (including all countries of the European Union and European Free Trade Area, and separately the United Kingdom). Incidence was aggregated over the Morbidity and Mortality Weekly Report (MMWR) epidemiological week definition of Sunday through Saturday. When predicting any single forecast target, teams could express uncertainty by submitting predictions across a range of a pre-specified set of 23 quantiles in the probability distribution. Teams could also submit a single point forecast without uncertainty. At the first submission we asked teams to add a single set of metadata briefly describing the forecasting team and methods. Model types included mechanistic models, agent-based and statistical models, ensembles of multiple models and an expertise-based model. We maintain a full project specification with a detailed submissions protocol [@europeancovid-19forecasthubCovid19forecasthubeuropeWiki]. 

With the complete dataset for the latest forecasting week available each Sunday, forecasts were typically submitted to the hub on Monday. We used an automated validation programme to check that each new forecast conformed to standardised formatting. This included checking that predictions increased monotonically with each increasing quantile, that predictions were integer counts, as well as that forecasts conformed to consistent date and location definitions. 

Each week we built an ensemble of all forecasts which was updated each week after all forecasts had been validated. From the first week of forecasting from 8 March 2021, the ensemble method for summarising across forecasts was the mean average of all models at each predictive quantile for a given location, target, and horizon. From 26 July 2021 onwards the ensemble instead used a median average of all predictive quantiles, in order to mitigate the wide uncertainty produced by some highly anomalous forecasts. We created an open and publicly accessible interface to the forecasts and ensemble, including an online visualization tool allowing viewers to see past data and interact with one or multiple forecasts for each country and target for up to four weeks' horizon [@europeancovid-19forecasthubEuropeanCovid19Forecast]. All forecast and meta data are freely available and held on Zoltar, a platform for hosting epidemiological forecasts (@epiforecastsProjectECDCEuropean2021; @reichZoltarForecastArchive2021). 

### Forecast evaluation

We evaluated all previous forecasts against actual observed values for each model, stratified by the forecast horizon, location, and target. We calculated scores using the _scoringutils_ R package [@nikosibosseScoringutilsUtilitiesScoring2020] with observed data reported by Johns Hopkins University (JHU, [@dongInteractiveWebbasedDashboard2020a]). JHU data included a mix of national and aggregated subnational data for the 32 countries in the Hub. We removed any forecast surrounding (in the week of or after) a strongly anomalous data point.

We assessed calibration via coverage of the predictive intervals and overall predictive performance via the weighted interval score (WIS). Coverage at a given interval level $k$ was calculated as the proportion $p$ of observations that fell within the corresponding central predictive intervals across locations and forecast dates. A perfectly calibrated model would have $p=k$ at all 11 levels (corresponding to 22 quantiles excluding the median). An underconfident model at level $k$ would have $p>k$, i.e. more observations falling within a given interval than expected, whereas an overconfident model at level $k$ would have $p<k$, i.e. fewer observations falling within a given interval than expected. We here focus on coverage at the 50% and 95% level.

We assessed weekly forecasts using the WIS across all quantiles that were being gathered[@bracherEvaluatingEpidemicForecasts2021]. This WIS is a strictly proper scoring rule, that is, it is optimised for predictions that come from the data-generating model and, as a consequence, encourages forecasters to report predictions representing their true belief about the future[@gneitingStrictlyProperScoring2007]. The WIS represents a parsimonious approach to scoring forecasts when only quantiles are available:

$$\mathrm{WIS}_{\alpha_{0:K}}(y, F) = \frac{1}{K + 0.5} \left( \frac{1}{2} \left| y - m \right| + \sum_{k=1}^{K} \left(\frac{\alpha_k}{2} \left(u_k - l_k \right) + \left( l_k - y \right) \mathbb{I}_{y < l_k} + \left( y - u_k \right) \mathbb{I}_{y > l_k} \right) \right)$$

Where $y$ is an observed outcome, $F$ the forecast, $m$ the median of the predictive distribution, $u_k$ and $l_k$ are the predictive upper and lower quantiles corresponding to the central predictive interval level $k$, respectively, $K=11$ is the number of intervals considered and $1 - \alpha_k$ the width of central predictive interval $k$. 

Not all models were used to generate forecasts for all locations and all forecast dates. In order to be able to compare predictive performance in the face of various levels of missingness, we further calculated a relative WIS as a relative measure of forecast performance which takes into account that different teams may not cover the exact same set of forecast targets (i.e., weeks and locations). Loosely speaking, a relative WIS of X means that averaged over the targets a given team addressed, its WIS was X times higher/lower than the the performance of the baseline model assuming case/death numbers to stay the same in the future, described previously[@cramerEvaluationIndividualEnsemble2021c]. Smaller values are thus better and a value below one means that the model has above average performance. The relative WIS is computed using a ?pairwise comparison tournament? where for each pair of models a mean score ratio is computed based on the set of shared targets. The relative WIS of a model with respect to another model is then the ratio of their respective geometric mean of the mean score ratios, where here we report the relative WIS of each model with respect to the baseline model.

#### Ensemble methods

We retrospectively explored alternative methods for ensembling forecasts for each target each week. A natural way to combine probability distributions available in a quantile format, such as the ones collated in the European COVID-19 Forecast Hub is[@genestVincentizationRevisited1992]
$$F^{-1}(\alpha) = \sum_{i=1}^{n}w_i F_i^{-1}(\alpha)$$
Where $F_{1 \ldots n}$ are the cumulative distribution functions of the individual probability distributions (in our case, the predictive distributions of the forecasts contributed to the hub), $w_i$ are a set of weights; and $\alpha$ are the quantile levels such that
$$F^{-1}(\alpha) = \mathrm{inf} \{t : F_i(t) \geq \alpha \}$$.
Different ensemble choices then largely come down to the choice of weights $w_i$.

The simplest choice of weights is to set them all equal so that they sum up to 1, $w_i=1/n$,, resulting in an unweighted mean ensemble. This can be subject to outlier forecasts that could skew the mean. A choice that avoids this potential issue is to combine the forecasts at each quantile level in an unweighted median ensemble, where the average is replaced by a median. An unweighted median ensemble has previously been found to yield very competitive performance while maintaining robustness to outlying forecast[@rayChallengesTrainingEnsembles2021]. 
By choosing the weights $w_i$ to reflect past performance one can move from an untrained to a trained ensemble. Numerous options exist for choosing the weights with the aim to maximise predictive performance. A straightforward choice is so-called inverse score weighting, which was recently found in the US to outperform unweighted scores during some time periods[@taylorCombiningProbabilisticForecasts2021] but not confirmed in a similar study in Germany and Poland [@bracherPreregisteredShorttermForecasting2021]. In this case, the weights are calculated as
$$w_i = \frac{1}{S_i}$$
where $S_i$ reflects the forecast skill of forecaster $i$.

Here we considered unweighted and inverse relative WIS weighted mean and median ensembles. We additionally considered ensembles where the training period was restricted to the last 10 weeks (for the weighted ensembles), or where only models with relative WIS < 1 (i.e., outperforming the baseline) were admitted included.

# Results

```{r model-description}
# Get latest evaluation scores for models
source(here("code", "get-model-eval.R"))

# Note - All model descriptions include the hub ensemble model
# Evaluation period
n_weeks <- max(model_eval$n)
start_date <- format.Date(eval_date - weeks(n_weeks), "%d %B")
end_date <- format.Date(eval_date, "%d %B %Y")

# Number of forecasters
n_model <- length(unique(model_eval$model))
n_team <- length(unique(model_eval$team_name))

# Number of models with rel wis scores
n_model_wis <- filter(model_eval, !is.na(rel_wis)) %>%
  distinct(model) %>% nrow()
```

We scored all forecasts submitted weekly in real time over the `r n_weeks` week period from `r start_date` to `r end_date`. Each week, forecasts were collated for incident cases and deaths, for 32 locations over the following 4 weeks, creating 256 possible forecast targets. We received `r n_model` unique forecasting models from `r n_team` separate forecasting teams. We added an ensemble model using all available forecasts for each possible target every week. 

We used this dataset to create `r nrow(model_eval)` forecasting scores, each summarising a unique combination of model, variable, country, and week ahead horizon. Not all teams forecast for all targets, nor across all quantiles of the predictive distribution for each target. `r n_model_wis` models provided sufficient quantiles that we could evaluate them using the relative weighted interval score (WIS).

```{r figure-1-model-by-horizon}
#| fig.cap = "Figure 1: Performance of short-term forecasts, by relative
#|  interval score (relative to a baseline forecast, top) and coverage of the central 50%
#|  interval (the proportion of observed values that fell within the predicted
#|  50% range, bottom). The scores of each model (grey) and ensemble (red) are
#|  averaged across all forecast targets and shown by one- to four-week ahead horizon."

# How often did the ensemble beat any score from all model scores?
n_ensemble <- filter(model_eval, !is_hub) %>%
  group_by(target_variable) %>%
  summarise(n = n(),
            ensemble_beat = sum(ensemble_rel_wis < rel_wis, na.rm = TRUE),
            p_ensemble_beat = ensemble_beat / n * 100)
n_model_scores <- map(split(n_ensemble, n_ensemble$target_variable),
                       ~ .x %>% pull(n))
p_ensemble_beat <- map(split(n_ensemble, n_ensemble$target_variable),
                       ~ .x %>% pull(p_ensemble_beat) %>% round(0))

# PLOT: all models and ensemble performance by line over 1:4 week horizon
# average scores across locations for each model
h1_h4 <- model_eval %>%
  group_by(is_hub, model, horizon, target_variable) %>%
  summarise(is_hub = any(is_hub),
            n = n(),
            mean_wis = mean(rel_wis, na.rm = TRUE),
            sd_wis = sd(rel_wis, na.rm = TRUE),
            mean_cov50 = mean(cov_50, na.rm = TRUE),
            mean_cov95 = mean(cov_95, na.rm = TRUE))

# rel WIS per model by horizon
plot_model_wis <- h1_h4 %>%
  ggplot(aes(x = horizon, y = mean_wis, colour = model)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = 1), lty = 2, col = "black") +
  # highlight ensemble in red and add label to plot
  gghighlight(is_hub, calculate_per_facet = TRUE, use_direct_label = F) +
  labs(y = "Mean relative WIS", x = NULL) +
  scale_color_brewer(type = "qual", palette = 6) +
  facet_wrap(~ target_variable, scales = "fixed") +
  coord_cartesian(ylim = c(0, 2)) +
  theme(strip.background = element_blank(),
        legend.position = "none",
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) 

# Plot coverage by horizon
plot_model_coverage <- h1_h4 %>%  
  # show only 0.5 coverage level in plot
  mutate(expected = 0.5) %>%
  ggplot(aes(x = horizon, y = mean_cov50, colour = model)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = expected), lty = 2, colour = "black") +
  gghighlight(is_hub, calculate_per_facet = TRUE, use_direct_label = F) +
  scale_color_brewer(type = "qual", palette = 6) +
  labs(y = "50% coverage", x = "Weeks ahead horizon",
       fill = NULL, colour = NULL) +
  facet_wrap(~ target_variable, scales = "fixed") +
  theme(strip.background = element_blank(),
        strip.text = element_blank(),
        legend.position = "bottom") 

# Plot together
plot_model_wis +
  plot_model_coverage +
  plot_layout(nrow = 2)

```

The ensemble model performed well compared to both its component models and the baseline. In ranking all models' scores compared to the baseline, the ensemble performed better on relative WIS than `r p_ensemble_beat$Cases`% model scores when forecasting cases (n=`r n_model_scores$Cases`), and `r p_ensemble_beat$Deaths`% of scores for forecasts of incident deaths (n=`r n_model_scores$Deaths`). Compared to the baseline model alone, the ensemble outperformed it at the one-week ahead horizon for both cases and deaths (figure 1). For horizons longer than one week, performance depended on the epidemiological target. The ensemble stopped outperforming the baseline at three to four weeks for cases. In contrast, the ensemble outperformed the baseline for deaths at all horizons considered (up to four weeks WIS), with no discernible deterioration in performance. 

We observed similar trends in performance when considering how well the ensemble was calibrated with respect to the observed data. At one week ahead the case ensemble was well calibrated (ca. 50% nominal coverage at the 50% level), but this did not hold with increasing forecast horizon. The death ensemble was well calibrated at the 95% level for all horizons and under confident for the 50% level, with only slow deterioration with increasing forecast horizon.

```{r figure-2-model-by-location}
#| fig.cap = "Figure 2: Performance of short-term forecasts across models and
#|  median ensemble (asterisk), by country, forecasting cases (top) and deaths
#|  (bottom) for two-week ahead forecasts, according to the relative weighted interval score. Boxplots show interquartile
#|  ranges, with outliers as faded points, and the ensemble model performance is marked
#|  by an asterisk."

# How often did the ensemble beat the baseline across locations?
n_ensemble_loc <- model_eval %>%
  filter(is_hub) %>%
  group_by(target_variable) %>%
  summarise(n = n(), # n = 32 countries for each target
            p_ensemble_beat_loc = sum(rel_wis <= 1, na.rm = TRUE) / n * 100)
p_ensemble_beat_loc <- map(split(n_ensemble_loc, n_ensemble_loc$target_variable),
                       ~ .x %>% pull(p_ensemble_beat_loc) %>% round(0))

# best individual model performance
n_model_loc <- model_eval %>%
  filter(!is_hub) %>%
  group_by(model, target_variable) %>%
  summarise(n = n(), # n = 32 countries for each target
            p_model_beat_loc = sum(rel_wis <= 1, na.rm = TRUE) / n * 100,
            .groups = "drop") %>%
  filter(n == unique(n_ensemble_loc$n)) %>%
  arrange(desc(p_model_beat_loc)) %>%
  group_by(target_variable) %>%
  slice_max(p_model_beat_loc, n = 1)
p_model_beat_loc <- map(split(n_model_loc, n_model_loc$target_variable),
                       ~ .x %>% 
                         mutate(p_model_beat_loc = round(p_model_beat_loc, 0)))

# PLOT: All model and ensemble performance by boxplot by location at 2 wk horizon
model_eval %>%
  # Use 2 week horizon
  filter(horizon == 2) %>%
  # plot structure: boxplot rel wis by location and horizon
  ggplot(aes(x = location_name, y = rel_wis,
             colour = target_variable,
             fill = target_variable)) +
  geom_boxplot(alpha = 0.8, 
               outlier.alpha = 0.2) +
  geom_hline(aes(yintercept = 1), lty = 2) +
  # overlay ensemble as extra point
  geom_point(aes(y = ensemble_rel_wis),
              size = 2, shape = "asterisk",
             colour = "grey 10",
             position = position_dodge(width = 0.8)) +
  # format
  ylim(c(0,4)) +
  labs(x = NULL, y = "Scaled relative WIS across models") +
  scale_fill_brewer(palette = "Set1") +
  scale_colour_brewer(palette = "Set1") +
  facet_wrap(~ target_variable, scales = "fixed", nrow = 2) +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 30, hjust = 1),
        strip.background = element_blank())
```

The ensemble also performed consistently well when forecasting across countries relative to individual models and the baseline (figure 2, figure SI1).  Compared to models that forecast across all 32 countries at the two-week horizon, the ensemble was more consistent in outperforming the baseline across countries compared to any single model forecasting deaths, and all but one model for case forecasts. Considering forecast targets across all 32 countries and over all four horizons (128 targets), the ensemble forecast outperformed the baseline for `r p_ensemble_beat_loc$Cases`% and `r p_ensemble_beat_loc$Deaths`% of all 128 targets when forecasting cases and deaths, respectively. Comparably, the best individual models forecasting for the same number of targets as the ensemble model outperformed the baseline for `r p_model_beat_loc$Cases$p_model_beat_loc`% ("`r p_model_beat_loc$Cases$model`" model) for cases and `r p_model_beat_loc$Deaths$p_model_beat_loc`% ("`r p_model_beat_loc$Deaths$model`" model) for death forecasts.

```{r figure-3-alternative-ensembles, out.height = "50%"}
#| fig.cap = paste0(
#| "Figure 3: Performance of alternative ensemble methods at 2 week
#|  horizon, showing mean difference (triangle) in relative weighted
#|  interval score, with 48% and 96% probability (thick and thin line
#|  respectively). The difference in WIS is a comparison of
#|  scores from forecasts made from all possible combinations of methods,
#|  with a single element of ensemble method input changed.
#|  Reference categories are: 
#|  weighted v. unweighted (n=", ensemble_change_n$Weighted, 
#|  "); median v. mean (n=", ensemble_change_n$Median, 
#|  "); cutoff by WIS v. all models included (n=", 
#|  ensemble_change_n$`Cutoff rel. WIS < 1`, 
#|  "); relative WIS measures over 10 weeks of forecast history vs. all forecasts (n=",
#|  ensemble_change_n$`10 weeks history`, ")")

# OR-style plot showing relative impact of different ensemble methods on performance
# get eval, calculate differences between ensemble methods
source(here("code", "get-ensemble-eval.R"))
ensemble_change <- compare_ensemble_diffs(at_horizon = 2,
                                          eval_ensemble = eval_ensemble)
ensemble_change_dtb <- ensemble_change$ensemble_change_dtb
ensemble_change_n <- ensemble_change$ensemble_change_n
# plot differences summary
ggplot(ensemble_change_dtb, aes(x = change)) +
  # mean = triangle
  geom_point(aes(y = mean), pch = 2, size = 4) +
  # 48% interval = thick line
  geom_linerange(aes(ymin = low_48, ymax = high_48), lwd = 2) +
  # 96% interval = thin line
  geom_linerange(aes(ymin = low_96, ymax = high_96), lwd = 1) +
  # 0 change in wis = reference line
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x = NULL, y = "Change in relative WIS compared to reference") +
  coord_flip()
```

At the two-week ahead horizon, variations in ensemble methods made little difference to forecast scores (figure 3, figure SI2). Ensembles that weighted forecasts showed no difference in performance to simple unweighted ensemble methods. Similarly, in choosing a method with which to weight forecasts, the choice of whether to use scores across all past forecasts, or scores evaluating only the most recent 10 weeks' forecast scores, made very little difference to the performance of the resulting ensemble (0 mean change in forecast score). The choice to exclude any forecast that scored worse than the baseline forecast ("cut off") affected the performance of the ensemble in both directions, overall slightly worsening performance (`r ensemble_change_dtb[ensemble_change_dtb$change == "Cutoff rel. WIS < 1", "mean"]` relative WIS).  Using the median average was the only variation of ensemble method that typically improved performance, compared to using the mean average across any combination of ensemble method.

# Discussion

We collated forecasts from multiple teams making forecasts of COVID-19 cases and deaths across countries over March to November in Europe, using an open and principled approach to standardising both forecast targets and the uncertainty around predictions. Combining these forecasts into an ensemble we found that the ensemble forecasts outperformed a baseline model at short forecast horizons and produced among the most consistent predictive performance across countries over time.

Our results support previous findings that ensembles are or are near the best performing models by error and are the most reliably consistent models in terms of appropriate coverage of uncertainty [@funkShorttermForecastsInform2020; @cramerEvaluationIndividualEnsemble2021c]. While the ensemble was consistently high performing, it was not strictly dominant across all forecast targets, with others also seeing this in comparable studies of COVID-19 forecasts [@bracherPreregisteredShorttermForecasting2021; @brooksComparingEnsembleApproaches2020]. This finding suggests the usefulness of an ensemble as a robust summary when forecasting across many spatio-temporal targets, without replacing the importance of communicating the full range of model predictions.

We can identify the benefits of an ensemble approach in light of the epidemic dynamics of COVID-19 in Europe over March to November. The introduction of vaccination changed the associations between infections, cases, and deaths [@europeancentrefordiseasepreventionandcontrolInterimGuidanceBenefits2021]. At the same time, the emergence of the delta variant altered transmission dynamics as it became the dominant viral strain across Europe [@europeancentrefordiseasepreventionandcontrolThreatAssessmentBrief2021]. However, neither of these factors were uniform across countries covered by the Forecast Hub [@europeancentrefordiseasepreventionandcontrolOverviewImplementationCOVID192021]. As epidemic dynamics became increasingly heterogeneous, the forecasting performance of any single model over time and across multiple countries became at least partly dependent on the ability, speed, and precision with which it could adapt to new conditions for each forecast target. This variability in the relative performance of models over time makes using an ensemble, balancing across all models, particularly relevant in rapidly changing epidemic conditions.

Our results also suggest the limited value of reporting case forecasts further than two weeks into the future. Previous work has similarly found rapidly declining performance for case forecasts with increasing horizon [@cramerEvaluationIndividualEnsemble2021c]. COVID-19 has a typical serial interval of less than a week, meaning that case forecasts of over two weeks can only hold if rates of transmission and detection remain predictable over the entire period, a strong assumption in the light of the many instances of rapidly changing policies and individual behaviour observed during the pandemic. 

In contrast, our ensemble, its component models, and previous work all highlight the more stable performance of death forecasts over time horizon. The ensemble in this study continued to outperform the baseline at four weeks ahead, and other work has found death forecasts perform well with up to six weeks lead time [@friedmanPredictivePerformanceInternational2021]. We could interpret this as due to the longer time lag between infection and death, which allows forecasters to incorporate the effect of changes in transmission. Additionally, the performance of trend-based forecasts may have benefited from the slower changes to trends in incident deaths caused by increasing vaccination rates, in turn supporting the performance of the ensemble. 

Exploring variations in ensemble methods we found that the choice of simple mean or median average had the most consistent impact on performance, regardless of methods of weighting and inclusion by performance history. Other work has supported the importance of the median in providing a stable forecast that better accounts for outliers than the mean [@brooksComparingEnsembleApproaches2020]. However, our results did not show a strong performance benefit for any one methodological choice, joining the existing mixed evidence for any optimal ensemble method for combining short term probabilistic infectious disease forecasts. In similar analyses of US COVID-19 forecasts many methods of combination have performed competitively, including the simple mean and weighted approaches outperforming unweighted or median methods [@taylorCombiningProbabilisticForecasts2021]. This contrasts with later analyses finding weighted methods to give similar performance to a median average [@brooksComparingEnsembleApproaches2020; @rayEnsembleForecastsCoronavirus2020e]. We can partly explain this inconsistency if performance of each method depends on the outcome being predicted (cases, deaths), its count (incident, cumulative) and absolute level, and the varying quality and quantity of forecasting teams over time. 

We also identified benefits of our approach beyond the results of this analysis. Open access to visualised forecasts and data is useful for both academics and the public in an emergency setting when forecasts can influence individual to international actions that change epidemic dynamics [@basshuysenThreeWaysWhich2021]. Existing participatory modelling efforts for COVID-19 have been useful for policy communication [@cdcCoronavirusDisease20192020], while multi-country efforts have included only single models adapted to country-specific parameters [@aguasModellingCOVID19Pandemic2020; @adibParticipatoryModellingApproach2021]. By expanding participation to many modelling teams, our work can create robust ensemble forecasts across Europe while allowing comparison across forecasts built with different interpretations of current data, on a like for like scale in real time. At the same time, collating time-stamped predictions ensures that we can test true out-of-sample performance of models and avoid retrospective claims of performance. Testing the limits of forecasting ability with these comparisons forms an important part of communicating any model-based prediction to decision makers. 

However, we experienced several limitations to our approach. First, our assessment of individual model performance may have been inaccurate due to limitations in the data source used. We saw some real time data revised retrospectively, introducing bias in either direction where the data used to create forecasts was not the same as that used to evaluate it. We mitigated this by excluding forecasts made at or for a time of missing, unreliable, or heavily revised data. We used a manual process for determining anomalous data, and if we did not detect where data revisions affected forecasts this could have created inaccurate forecast scores. However, we note that the national data used here are less likely to see revisions than subnational data [@dongInteractiveWebbasedDashboard2020a]. 

The results presented also depend on our choice of performance metric. While other work supports the use of the weighted interval score [@bracherEvaluatingEpidemicForecasts2021, @gneitingStrictlyProperScoring2007; @taylorCombiningProbabilisticForecasts2021], our use of a flat-line comparison meant that it was more difficult for forecasts to perform well in relative terms during periods where incidence was very stable [@cramerEvaluationIndividualEnsemble2021c]. This may have differentially biased forecast performance where, for equally good forecasts for different targets, models that predicted a change in trend were rewarded with better scores than those that equally accurately predicted a stable continuation. Further work could consider how well our results compare when using an alternative baseline suitable for epidemics, for example an exponential growth model.

The result that the ensemble was among the most reliable across countries and over time could also have been influenced by the sample of contributing forecasts. We accepted all modelling teams' participation and teams used a wide variety of methods. Meanwhile, teams may have changed their forecast methods, and entered and exited the hub over time. The ensemble therefore included forecasts based on models with changing assumptions each week, and we did not test how far the stability or methods of component forecasts influenced the resulting ensemble. This could be significant, for example where in a time of low incidence, including only compartmental models in an ensemble improved predictive performance relative to including forecasts from a wider variety of methods [@taylorCombiningProbabilisticForecasts2021]. However, the same study found the most consistent ensemble over time was that which included all forecasts regardless of method, with performance increasing with the number of forecast models, so our results are unlikely to have changed by excluding any contributing forecasts.

We see additional scope to adapt the hub to the changing COVID-19 situation across Europe. We have recently extended the hub infrastructure to include short term forecasts for hospitalisations with COVID-19, although this faces additional challenges with limited data across the locations covered by the hub. It may also be valuable to separately investigate models for longer term scenarios in addition to the short term forecasts, particularly as the policy focus shifts from immediate response to anticipating changes brought by vaccinations or the geographic spread of new variants [@europeancentrefordiseasepreventionandcontrolOverviewImplementationCOVID192021]. 

This study raises further questions which could inform epidemic forecast modellers and users. We recommend using the dataset created by the European Forecast Hub for further research on forecast performance. Future work could explore the impact of changing epidemiology on individual or ensemble models by combining analyses of trends and turning points in cases and deaths with forecast performance, or extending to include data on vaccination, variant, or policy changes over time. There is also a wide range of methods for combining forecasts which could improve performance of an ensemble or continue to demonstrate the value of a simple approach. This includes altering the inclusion criteria of forecast models based on different thresholds of past performance, excluding or including only forecasts that predict the lowest- and highest-values (trimming) [@taylorCombiningProbabilisticForecasts2021], or using alternative weighting methods such as quantile regression averaging [@funkShorttermForecastsInform2020]. Exploring these questions would add to our understanding of real time performance, supporting and improving future forecasting efforts.

We further recommend adapting and using our open-source computational infrastructure elsewhere for applied public health work. The hub structure maximises the transparency and accuracy of real-time forecasts and can reduce reliance on individual models as a basis for action during an epidemic. The benefits of combining multiple models into an ensemble come from individual models' wide variation in forecast performance across varying targets, and this is particularly true during emerging epidemics where forecasters vary in how quickly their models are able to adapt to new information. Setting up the infrastructure for this could be an important component to future epidemic and pandemic preparedness.

In conclusion, we have shown that an ensemble forecast performed reliably well across multiple forecast targets, with good short term predictions during a rapidly evolving epidemic spreading through multiple populations. However we have also demonstrated there are clear limits to predictability, especially for case forecasts longer than two weeks, with few methods able to consistently improve forecast performance other than the use of a median rather than mean average.

# References

<div id="refs"></div>

# Supplementary information

```{r si-figure-model-by-location, out.width = "150%"}
#| fig.cap = "Figure SI1: Performance of short-term forecasts across models and
#|  median ensemble (asterisk), by country, forecasting cases (left) and deaths
#|  (right) for one-week (top) through four-week (bottom) ahead forecasts, 
#|  according to the relative weighted interval score. Boxplots show interquartile
#|  ranges, with outliers as faded points, and the ensemble model performance is marked
#|  by an asterisk."

model_eval %>%
  # plot structure: boxplot rel wis by location and horizon
  ggplot(aes(x = location_name, y = rel_wis,
             colour = target_variable,
             fill = target_variable)) +
  geom_boxplot(alpha = 0.8, 
               outlier.alpha = 0.2) +
  geom_hline(aes(yintercept = 1), lty = 2) +
  # overlay ensemble as extra point
  geom_point(aes(y = ensemble_rel_wis),
              size = 2, shape = "asterisk",
             colour = "grey 10",
             position = position_dodge(width = 0.8)) +
  # format
  ylim(c(0,4)) +
  labs(x = NULL, y = "Scaled relative WIS across models") +
  scale_fill_brewer(palette = "Set1") +
  scale_colour_brewer(palette = "Set1") +
  facet_grid(cols = vars(target_variable), 
             rows = vars(horizon),
             scales = "fixed") +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 30, hjust = 1),
        strip.background = element_blank())
```

```{r si-figure-alternative-ensembles, out.height = "30%"}
#| fig.cap = "Figure SI2: Performance of alternative ensemble methods at each weekly
#|  horizon (1-4), showing mean difference (triangle) in relative weighted
#|  interval score, with 48% and 96% probability (thick and thin line
#|  respectively). The difference in WIS is a comparison of
#|  scores from forecasts made from all possible combinations of methods,
#|  with a single element of ensemble method input changed.
#|  Reference categories are: 
#|  weighted v. unweighted; median v. mean; 
#|  cutoff by WIS v. all models included;, 
#|  relative WIS measures over 10 weeks of forecast history vs. all forecasts"


# OR-style plot showing relative impact of different ensemble methods on performance
# get eval, calculate differences between ensemble methods
source(here("code", "get-ensemble-eval.R"))
# ensemble_change <- compare_ensemble_diffs(at_horizon = 2,
#                                           eval_ensemble = eval_ensemble)
# ensemble_change_dtb <- ensemble_change$ensemble_change_dtb
# ensemble_change_n <- ensemble_change$ensemble_change_n

ensemble_change_dtb_all <- map_dfr(1:4, 
                          ~ compare_ensemble_diffs(at_horizon = .x,
                          eval_ensemble = eval_ensemble)$ensemble_change_dtb)

# plot differences summary
ggplot(ensemble_change_dtb_all, aes(x = change)) +
  # mean = triangle
  geom_point(aes(y = mean), pch = 2, size = 4) +
  # 48% interval = thick line
  geom_linerange(aes(ymin = low_48, ymax = high_48), lwd = 2) +
  # 96% interval = thin line
  geom_linerange(aes(ymin = low_96, ymax = high_96), lwd = 1) +
  # 0 change in wis = reference line
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x = NULL, y = "Change in relative WIS compared to reference") +
  coord_flip() +
  facet_wrap(~ horizon, nrow = 1)
```
