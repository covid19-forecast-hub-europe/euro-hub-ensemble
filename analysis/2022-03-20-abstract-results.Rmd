---
title: "Predictive performance of multi-model ensemble forecasts of COVID-19 across European nations"
output:
  html_document: default
  pdf_document: default
bibliography: references.bib
link-citations: true
csl: ieee.csl
---

```{r set up, include=FALSE}
# Packages
library(here)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(ggplot2)
library(forcats)
library(patchwork)
library(gghighlight)
# Settings
this_doc <- "analysis/results.Rmd"
here::i_am(this_doc)
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, 
                      message = FALSE, warning = FALSE,
                      eval.after = "fig.cap")
theme_set(theme_bw())

# Get latest evaluation scores for models
source(here("code", "get-model-eval.R"))

# Note - All model descriptions include the hub ensemble model
# Evaluation period
n_weeks <- max(model_eval$n)
start_date <- format.Date(eval_date - weeks(n_weeks), "%d %B %Y")
end_date <- format.Date(eval_date, "%d %B %Y")
```

```{r model-description}
# Number of forecasters
n_model <- length(unique(model_eval$model))
n_team <- length(unique(model_eval$team_name))

# Number of models with rel wis scores
n_model_wis <- filter(model_eval, !is.na(rel_wis)) %>%
  distinct(model) %>% nrow()

# How often did the ensemble beat any score from all model scores?
n_ensemble <- filter(model_eval, !is_hub) %>%
  group_by(target_variable) %>%
  summarise(n = n(),
            ensemble_beat = sum(ensemble_rel_wis < rel_wis, na.rm = TRUE),
            p_ensemble_beat = ensemble_beat / n * 100)
n_model_scores <- map(split(n_ensemble, n_ensemble$target_variable),
                       ~ .x %>% pull(n))
p_ensemble_beat <- map(split(n_ensemble, n_ensemble$target_variable),
                       ~ .x %>% pull(p_ensemble_beat) %>% round(0))
```

```{r figure-1-box-by-horizon}
models_ensemble_cols <- c("#9ebcda", "#8856a7")

# Ensemble performance in 32 locations plotted at each horizon
horizon <- model_eval %>%
  mutate(horizon = factor(horizon),
         Model = factor(is_hub, levels = c(FALSE, TRUE),
                        labels = c("All other models", "Hub ensemble")))

# Rel wis
rel_wis <- horizon %>%
  ggplot(aes(y = rel_wis,
             x = horizon,
             col = Model)) +
  geom_boxplot(varwidth = TRUE, outlier.alpha = 0.2) +
  geom_hline(aes(yintercept = 1), lty = 2) +
  annotate("rect", alpha = 0.1, fill = "gold",
           xmin = -Inf, xmax = Inf,
           ymin = -Inf, ymax = 1) +
  coord_cartesian(ylim = c(0, 2)) +
  labs(y = "Scaled relative WIS",
       x = NULL, col = NULL) +
  scale_colour_manual(values = models_ensemble_cols) +
  facet_wrap(~ target_variable, ncol = 2) +
  theme(strip.background = element_blank(),
        legend.position = "none",
        axis.text.x = element_blank())

# 50% coverage
cov_50 <- horizon %>%
  ggplot(aes(y = cov_50,
             x = horizon, col = Model)) +
  geom_boxplot(varwidth = TRUE, outlier.alpha = 0.2) +
  geom_hline(aes(yintercept = 0.5), lty = 2) +
  annotate("rect", alpha = 0.1, fill = "gold",
           xmin = -Inf, xmax = Inf,
           ymin = 0.45, ymax = 0.55) +
  coord_cartesian(ylim = c(0, 1)) +
  labs(y = "50% coverage",
       x = NULL, col = NULL) +
  facet_wrap(~ target_variable, ncol = 2) +
  scale_colour_manual(values = models_ensemble_cols) +
  theme(strip.text = element_blank(),
        strip.background = element_blank(),
        legend.position = "none",
        axis.text.x = element_blank())

# 95% coverage
cov_95 <- horizon %>%
  ggplot(aes(y = cov_95,
             x = horizon, col = Model)) +
  geom_boxplot(varwidth = TRUE, outlier.alpha = 0.2) +
  geom_hline(aes(yintercept = 0.95), lty = 2) +
  annotate("rect", alpha = 0.1, fill = "gold",
           xmin = -Inf, xmax = Inf,
           ymin = 0.90, ymax = 1) +
  coord_cartesian(ylim = c(0, 1)) +
  labs(y = "95% coverage",
       x = "Weeks ahead horizon",
       col = NULL,
       caption = "Boxplot width proportional to observations;
       Yellow indicates target range") +
  facet_wrap(~ target_variable, ncol = 2) +
  scale_colour_manual(values = models_ensemble_cols) +
  theme(strip.text = element_blank(),
        strip.background = element_blank(),
        legend.position = "bottom")
figure_1 <-
  rel_wis +
  cov_50 +
  cov_95 +
  plot_layout(ncol = 1)

ggsave(filename = here("output", "figures", "figure-1-horizon.png"), plot = figure_1, width = 5, height = 7)
```

```{r figure-2-model-by-location}
#| fig.cap = "Performance of short-term forecasts across models and
#|  median ensemble (asterisk), by country, forecasting cases (top) and deaths
#|  (bottom) for two-week ahead forecasts, according to the relative weighted interval score. 
#|  Boxplots show interquartile ranges, with outliers as faded points, and the 
#|  ensemble model performance is marked by an asterisk.
#|  y-axis is cut-off to an upper bound of 4 for readability, excluding one data point
#|  (relative WIS 5.91, one model of cases in Netherlands)"

# How often did the ensemble beat the baseline across locations?
n_ensemble_loc <- model_eval %>%
  filter(is_hub) %>%
  group_by(target_variable) %>%
  summarise(n = n(), # n = 32 countries for each target
            p_ensemble_beat_loc = sum(rel_wis <= 1, na.rm = TRUE) / n * 100)
p_ensemble_beat_loc <- map(split(n_ensemble_loc, n_ensemble_loc$target_variable),
                       ~ .x %>% pull(p_ensemble_beat_loc) %>% round(0))

# best individual model performance
n_model_loc <- model_eval %>%
  filter(!is_hub) %>%
  group_by(model, target_variable) %>%
  summarise(n = n(), # n = 32 countries for each target
            p_model_beat_loc = sum(rel_wis <= 1, na.rm = TRUE) / n * 100,
            .groups = "drop") %>%
  filter(n == unique(n_ensemble_loc$n)) %>%
  arrange(desc(p_model_beat_loc)) %>%
  group_by(target_variable) %>%
  slice_max(p_model_beat_loc, n = 1)
p_model_beat_loc <- map(split(n_model_loc, n_model_loc$target_variable),
                       ~ .x %>% 
                         mutate(p_model_beat_loc = round(p_model_beat_loc, 0)))

# PLOT: All model and ensemble performance by boxplot by location at 2 wk horizon
figure_2 <- model_eval %>%
  # Use 2 week horizon
  filter(horizon == 2) %>%
  mutate(rel_wis = ifelse(rel_wis == 0, NA, rel_wis)) %>%
  # plot structure: boxplot rel wis by location and horizon
  ggplot(aes(x = location_name, y = rel_wis,
             colour = target_variable,
             fill = target_variable)) +
  geom_boxplot(alpha = 0.8, 
               outlier.alpha = 0.2) +
  geom_hline(aes(yintercept = 1), lty = 2) +
  # overlay ensemble as extra point
  geom_point(aes(y = ensemble_rel_wis),
              size = 2, shape = "asterisk",
             colour = "grey 10",
             position = position_dodge(width = 0.8)) +
  # format
  ylim(c(0,4)) +
  labs(x = NULL, y = "Scaled relative WIS across models") +
  scale_fill_brewer(palette = "Set1") +
  scale_colour_brewer(palette = "Set1") +
  facet_wrap(~ target_variable, scales = "fixed", nrow = 2) +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 30, hjust = 1),
        strip.background = element_blank())

ggsave(filename = here("output", "figures", "figure-2-location.png"), plot = figure_2)
```
_Order tbc;_ Katharine Sherratt, Hugo Gruson, _Any co-authors_, _Team authors_, _Advisory team authors_, _ECDC authors_, Johannes Bracher, Sebastian Funk

# Abstract

_Background_ Short-term forecasts of infectious disease burden can contribute to situational awareness and aid capacity planning. Based on best practice in other fields and recent insights in infectious disease epidemiology, one can maximise the predictive performance of such forecasts if multiple models are combined into an ensemble. Here we report on the performance of ensembles created from `r #?` models in predicting COVID-19 cases and deaths across Europe between `r start_date` and `r end_date`.

_Methods_ We used open-source tools to develop a public European COVID-19 Forecast Hub. We invited groups globally to contribute weekly forecasts for COVID-19 cases and deaths over the next one to four weeks. Forecasts were submitted using standardised quantiles of the predictive distribution. Each week we created an ensemble forecast, where each predictive quantile was calculated as the equally-weighted average (initially the mean and then the median from the 26th of July) of all individual models’ predictive quantiles. We retrospectively explored alternative methods for ensemble forecasts, including weighted averages based on models’ past predictive performance. The performance of the ensembles was compared to individual models and a baseline model of no change using pairwise comparison of the Weighted Interval Score (WIS).

_Results_ Over `r #?` weeks we collected and combined `r #?` forecast models for `r #?` countries. We found a weekly ensemble had among the most reliable performance across countries over time, with more accurate predictions for reported cases and deaths than a simple baseline for `r #?` of `r #?` and `r #?` of `r #?` forecast targets respectively. Ensemble performance _declined with increasing forecast time horizon when forecasting cases but remained stable for 4 weeks for incident death forecasts_. _Among several choices of ensemble methods we found that the most influential and best choice was to use a median average of models instead of using the mean, regardless of methods of weighting component forecast models._

_Conclusions_ Our results support the use of combining forecasts from individual models into an ensemble in order to improve predictive performance across epidemiological targets and populations during infectious disease epidemics. Our findings suggested that for an emerging pathogen with many individual models, median ensemble methods may improve predictive performance more than mean ensemble methods. Our findings also highlight that forecast consumers should place more weight on incident death forecasts versus incident case forecasts for forecast horizons greater than two weeks.

_Code and data availability_ All data and code are publicly available on Github: covid19-forecast-hubeurope/euro-hub-ensemble. This document was generated on `r Sys.Date()`.

# Background

Epidemiological forecasts make quantitative statements about a disease outcome in the near future. Forecasting targets can include measures of prevalent or incident disease and its severity, for some population over a specified time horizon. Researchers, policy makers, and the general public have used such forecasts to understand and respond to the global outbreaks of COVID-19 since early 2020 [@basshuysenThreeWaysWhich2021]. Forecasters use a variety of methods and models for creating and publishing forecasts, varying in both defining the forecast outcome and in reporting the probability distribution of outcomes [@zelnerAccountingUncertaintyPandemic2021; @jamesUseMisuseMathematical2021]. Such variation between forecasts makes it difficult to compare predictive performance between forecast models. These barriers to comparing and evaluating forecasts make it difficult to derive objective arguments for using one forecast over another. This hampers the selection of a representative forecast and hinders finding a reliable basis for decisions.

A “forecast hub” is a centralised effort to improve the transparency and usefulness of forecasts, by standardising and collating the work of many independent teams producing forecasts [@reichCollaborativeMultiyearMultimodel2019]. A hub sets a commonly agreed-upon structure for forecast targets, such as type of disease event, spatio-temporal units, or the set of quantiles of the probability distribution to include from probabilistic forecasts. For instance, a hub may collect predictions of the total number of cases reported in a given country for each day in the next two weeks. Forecasters can adopt this format and contribute forecasts for centralised storage in the public domain. This shared infrastructure allows forecasts produced from diverse teams and methods to be visualised and quantitatively compared on a like-for-like basis, which can strengthen public and policy use of disease forecasts [@cdcCoronavirusDisease20192020]. The underlying approach to creating a forecast hub was pioneered for forecasting influenza in the USA and adapted for forecasts of short-term COVID-19 cases and deaths in the US [@rayEnsembleForecastsCoronavirus2020e] `#ADD ZOTERO cite US data descriptor paper`, with similar efforts elsewhere [@bracherPreregisteredShorttermForecasting2021; @funkShorttermForecastsInform2020; @bicherSupportingCOVID19PolicyMaking2021].
Standardising forecasts allows for combining multiple forecasts into a single ensemble with the potential for an improved predictive performance. Evidence from previous efforts in multi-model infectious disease forecasting suggests that forecasts from an ensemble of models can be consistently high performing compared to any one of the component models [@reichAccuracyRealtimeMultimodel2019; @johanssonOpenChallengeAdvance2019; @viboudRAPIDDEbolaForecasting2018]. Elsewhere, weather forecasting has a long-standing use of building ensembles of models using diverse methods with standardised data and formatting in order to improve performance [@buizzaIntroductionSpecialIssue2019; @moranEpidemicForecastingMessier2016].
The European COVID-19 Forecast Hub [@europeancovid-19forecasthubEuropeanCOVID19Forecast2021] is a project to collate short term forecasts of COVID-19 across 32 countries in the European region. The Hub is funded and supported by the European Centre for Disease Prevention and Control (ECDC), with the primary aim to provide reliable information about the near-term epidemiology of the COVID-19 pandemic to the research and policy communities and the general public. Second, the Hub aims to create infrastructure for storing and analysing epidemiological forecasts made in real time by diverse research teams and methods across Europe. Third, the Hub aims to maintain a community of infectious disease modellers underpinned by open science principles. We started formally collating and combining contributions to the European Forecast Hub in March 2021. Here, we investigate the predictive performance of an ensemble of all forecasts contributed to the Hub in real time each week, as well as the performance of variations of ensemble methods created retrospectively.

# Methods

We developed infrastructure to host and analyse forecasts, focussing on compatibility with the US [@cramerReichlabCovid19forecasthubRelease2021; @wangReichlabCovidHubUtilsRepository2021] and the German and Polish COVID-19 [@bracherGermanPolishCOVID192020] forecast hubs. 

### Forecast targets and standardisation 

We sought forecasts for two measures of COVID-19 incidence: the total reported number of cases and deaths per week. We considered forecasts for 32 countries in Europe, including all countries of the European Union and European Free Trade Area, and the United Kingdom. We compared forecasts against observed data reported by Johns Hopkins University (JHU, [@dongInteractiveWebbasedDashboard2020]). JHU data included a mix of national and aggregated subnational data for the 32 countries in the Hub. Incidence was aggregated over the Morbidity and Mortality Weekly Report (MMWR) `# ADD ZOTERO cite MMWR week` epidemiological week definition of Sunday through Saturday. When predicting any single forecast target, teams could express uncertainty by submitting predictions across a range of a pre-specified set of 23 quantiles in the probability distribution. Teams could also submit a single point forecast without uncertainty. At the first submission we asked teams to add a single set of metadata briefly describing the forecasting team and methods. No restrictions were placed on who could submit forecasts, and to increase participation we actively contacted known forecasting teams across Europe and the US and advertised among the ECDC network. Teams submitted a broad spectrum of model types, ranging from mechanistic to empirical models, agent-based and statistical models, and ensembles of multiple quantitative or qualitative models (`# see SI`). We maintain a full project specification with a detailed submissions protocol [@europeancovid-19forecasthubCovid19forecasthubeuropeWiki]. 
With the complete dataset for the latest forecasting week available each Sunday, teams typically submitted forecasts to the hub on Monday. We implemented an automated validation programme to check that each new forecast conformed to standardised formatting. The validation step ensured a monotonic increase of predictions with each increasing quantile, integer-valued counts of predicted cases, as well as consistent date and location definitions.
Each week we built an ensemble of all forecasts updated after all forecasts had been validated. From the first week of forecasting from 8 March 2021, the ensemble method for summarising across forecasts was the arithmetic mean of all models at each predictive quantile for a given location, target, and horizon. From 26 July 2021 onwards the ensemble instead used a median of all predictive quantiles, in order to mitigate the wide uncertainty produced by some highly anomalous forecasts. We created an open and publicly accessible interface to the forecasts and ensemble, including an online visualisation tool allowing viewers to see past data and interact with one or multiple forecasts for each country and target for up to four weeks’ horizon[@europeancovid-19forecasthubEuropeanCovid19Forecast].  All forecast and meta data are freely available and held on Zoltar, a platform for hosting epidemiological forecasts [@epiforecastsProjectECDCEuropean2021; @reichZoltarForecastArchive2021].

### Forecast evaluation

We evaluated all previous forecasts against actual observed values for each model, stratified by the forecast horizon, location, and target. We calculated scores using the scoringutils R package [@nikosibosseScoringutilsUtilitiesScoring2020]. We removed any forecast surrounding (in the week of the first week after) a strongly anomalous data point. We defined anomalous as where any subsequent data release revised that data point by over 5%. `# ADD number of excluded targets due to data revisions`

For each model, we established its overall predictive performance using the weighted interval score (WIS) and the accuracy of its prediction boundaries as the coverage of the predictive intervals. We calculated coverage at a given interval level k, where $k\in[0,1]$, as the proportion $p$ of observations that fell within the corresponding central predictive intervals across locations and forecast dates. A perfectly calibrated model would have $p=k$ at all 11 levels (corresponding to 22 quantiles excluding the median). An under confident model at level $k$ would have $p>k$, i.e. more observations fall within a given interval than expected. In contrast, an overconfident model at level $k$ would have $p<k$, i.e. fewer observations fall within a given interval than expected. We here focus on coverage at the $k=0.5$ and $k=0.95$ level.

We assessed weekly forecasts using the WIS, across all quantiles that were being gathered [@bracherEvaluatingEpidemicForecasts2021]. The WIS is a strictly proper scoring rule, that is, it is optimised for predictions that come from the data-generating model. As a consequence, the WIS encourages forecasters to report predictions representing their true belief about the future [@gneitingStrictlyProperScoring2007]. The WIS represents an approach to scoring forecasts based on uncertainty represented as forecast values across a set of quantiles [@bracherEvaluatingEpidemicForecasts2021]. The WIS represents  a parsimonious approach to scoring forecasts when only quantiles are available. Each forecast for a given location and date is scored based on an observed count of weekly incidence, the median of the predictive distribution and the width of the predictive upper and lower quantiles corresponding to the central predictive interval level (see [@bracherEvaluatingEpidemicForecasts2021]).
As not all models provided forecasts for all locations and dates, to compare predictive performance in the face of various levels of missingness, we calculated a relative WIS. This is a measure of forecast performance which takes into account that different teams may not cover the same set of forecast targets (i.e., weeks and locations). Loosely speaking, a relative WIS of x means that averaged over the targets a given team addressed, its WIS was x times higher or lower than the performance of the baseline model. Smaller values in the relative WIS are thus better and a value below one means that the model has above average performance. The relative WIS is computed using a _pairwise comparison tournament_ `#add more detail` where for each pair of models a mean score ratio is computed based on the set of shared targets. The relative WIS of a model with respect to another model is then the ratio of their respective geometric mean of the mean score ratios. 
We then took the relative WIS of each model and scaled this against the relative WIS of a baseline model, for each forecast target, location, date, and horizon. The baseline model assumes case or death counts stay the same as the latest data point over all future horizons, with expanding uncertainty, described previously in [@cramerEvaluationIndividualEnsemble2021]. Here we report the relative WIS of each model with respect to the baseline model.

#### Ensemble methods

We retrospectively explored alternative methods for combining forecasts for each target at each week. A natural way to combine probability distributions available in a quantile format, such as the ones collated in the European COVID-19 Forecast Hub, is [@genestVincentizationRevisited1992]
$$F^{-1}(\alpha) = \sum_{i=1}^{n}w_i F_i^{-1}(\alpha)$$

Where $F_{1} \ldots F_{n}$ are the cumulative distribution functions of the individual probability distributions (in our case, the predictive distributions of each forecast model $i$ contributed to the hub), $w_i$ are a set of weights in $[0,1]$; and $\alpha$ are the quantile levels such that

$$F^{-1}(\alpha) = \mathrm{inf} \{t : F_i(t) \geq \alpha \}$$

Different ensemble choices then mainly translate to the choice of weights $w_i$.
The simplest choice of weights $w_i$ is to set them all equal so that they sum up to 1, $w_i=1/n$, resulting in an arithmetic mean ensemble. However, with this method a single outlier can have a very strong effect on the ensemble forecast. To avoid this overrepresentation, we can choose a set of weights to apply to forecasts before they are combined at each quantile level. Numerous options exist for choosing these weights with the aim to maximise predictive performance, including choosing weights to reflect each forecast’s past performance (thereby moving from an untrained to a trained ensemble). A straightforward choice is so-called inverse score weighting, which was recently found in the US to outperform unweighted scores during some time periods [@taylorCombiningProbabilisticForecasts2021] but not confirmed in a similar study in Germany and Poland Poland [@bracherPreregisteredShorttermForecasting2021]. In this case, the weights are calculated as
$$w_i = \frac{1}{S_i}$$

where $S_i$ reflects the forecast skill of forecaster $i$, normalised so that weights sum to 1. 

Alternatively, previous research has found that an unweighted median ensemble, where the arithmetic mean of each quantile is replaced by a median, yields very competitive performance while maintaining robustness to outlying forecasts `# cite https://arxiv.org/abs/2201.12387`. Building on this, it is possible to use the same weights described above to create a weighted median. This uses the Harrel-Davis quantile estimator with a beta function to approximate the weighted percentiles `#ADD ZOTERO cite Harrell, F.E. & Davis, C.E. (1982). A new distribution-free quantile estimator. Biometrika, 69(3), 635-640` ; `# ADD ZOTERO cite https://www.rdocumentation.org/packages/cNORM/versions/2.0.3/topics/weighted.quantile` . Here we considered unweighted and inverse relative WIS weighted mean and median ensembles.

# Results

We scored all forecasts submitted weekly in real time over the `r n_weeks` week period from `r start_date` to `r end_date`. Each week, forecasts were collated for incident cases and deaths, for 32 locations over the following 4 weeks, creating 256 possible forecast targets each week. We received `r n_model` unique forecasting models from `r n_team` separate forecasting teams, plus our ensemble model using all available forecasts for each possible target every week. 

We used this dataset to create `r nrow(model_eval)` forecasting scores, each summarising a unique combination of model, variable, country, and week ahead horizon. All data are available: `# ADD data DOI`. Not all teams forecast for all targets, nor across all quantiles of the predictive distribution for each target. Of `r n_team` teams, `r # all_targets` forecast for all countries/targets.  `r n_model_wis` models provided the full set of 23 quantiles meaning that we could evaluate them using the relative weighted interval score (WIS).

```{r print-figure-1, fig.width=5, fig.height=7}
figure_1
```

The hub’s weekly created ensemble (all models, unweighted using a mean and then a median from July; the ensemble from now on) performed well compared to both its component models and the baseline. In ranking all models’ scores compared to the baseline, this ensemble performed better on relative WIS than `r p_ensemble_beat$Cases`% model scores when forecasting cases ($n=`r n_model_scores$Cases`$), and `r p_ensemble_beat$Deaths`% of scores for forecasts of incident deaths ($n=`r n_model_scores$Deaths`$). The ensemble outperformed the baseline model at the one-week ahead horizon for both cases and deaths (Figure 1). For horizons longer than one week, performance depended on the epidemiological target. The ensemble stopped outperforming the baseline at three to four weeks with respect to incidence cases. In contrast, the ensemble outperformed the baseline for deaths at all horizons considered (up to four weeks WIS), with no discernible deterioration in performance relative to the baseline.

We observed similar trends in performance when considering how well the ensemble was calibrated with respect to the observed data. At one week ahead the case ensemble was well calibrated (ca. 50% nominal coverage at the 50% level), but this did not hold with increasing forecast horizon. The death ensemble was well calibrated at the 95% level for all horizons and under confident for the 50% level, with only slow deterioration with increasing forecast horizon.

```{r print-figure-2}
figure_2
```

The ensemble also performed consistently well when forecasting across countries relative to individual models and the baseline (figure 2, figure SI1).  Compared to models that forecast across all 32 countries at the two-week horizon, the ensemble was more consistent in outperforming the baseline across countries compared to any single model forecasting deaths, and all but one model for case forecasts. Considering forecast targets across all 32 countries and over all four horizons (128 targets), the ensemble forecast outperformed the baseline for `r p_ensemble_beat_loc$Cases`% and `r p_ensemble_beat_loc$Deaths`% of all 128 targets when forecasting cases and deaths, respectively. 

#### performance of ensemble re. variants
_delta and omicron – show forecasts_

#### Alternative ensembles
_Weighted mean, weighted median, mean, median – table comparing performance_

