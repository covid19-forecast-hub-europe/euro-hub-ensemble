---
title: 'Results: main'
output:
  html_document: default
  pdf_document: default
---

```{r set up, include=FALSE}
# Packages
library(here)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(ggplot2)
library(forcats)
library(patchwork)
library(gghighlight)
# Settings
this_doc <- "analysis/results.Rmd"
here::i_am(this_doc)
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, 
                      message = FALSE, warning = FALSE,
                      eval.after = "fig.cap")
theme_set(theme_bw())

# Get latest evaluation scores for models
source(here("code", "get-model-eval.R"))

# Note - All model descriptions include the hub ensemble model
# Evaluation period
n_weeks <- max(model_eval$n)
start_date <- format.Date(eval_date - weeks(n_weeks), "%d %B %Y")
end_date <- format.Date(eval_date, "%d %B %Y")
```

```{r model-description}
# Number of forecasters
n_model <- length(unique(model_eval$model))
n_team <- length(unique(model_eval$team_name))

# Number of models with rel wis scores
n_model_wis <- filter(model_eval, !is.na(rel_wis)) %>%
  distinct(model) %>% nrow()
```

- We scored all forecasts submitted weekly in real time over the `r n_weeks` week period from `r start_date` to `r end_date`. 

Each week, forecasts were collated for incident cases and deaths, for 32 locations over the following 4 weeks, creating 256 possible forecast targets.

A single model forecasting for every location, target, and horizon could therefore submit up to `r (n_weeks*32*2*4)` forecasts.We excluded from evaluation forecasts made for `r nrow(anomalies)` targets where data for these targets were subsequently revised by >5%. 

- We received `r n_model` unique forecasting models from `r n_team` separate forecasting teams. 

We added an ensemble model using all available forecasts for each possible target every week. 

- We used this dataset to create `r nrow(model_eval)` forecasting scores, each summarising a unique combination of model, variable, country, and week ahead horizon. 

Not all teams forecast for all targets, nor across all quantiles of the predictive distribution for each target. 

- `r n_model_wis` models provided sufficient quantiles that we could evaluate them using the relative weighted interval score (WIS).

```{r figure-1-model-by-horizon}
#| fig.cap = "Performance of short-term forecasts, by relative
#|  interval score (relative to a baseline forecast, top) and coverage of the central 50%
#|  interval (the proportion of observed values that fell within the predicted
#|  50% range, bottom). The scores of each model (grey) and ensemble (red) are
#|  averaged across all forecast targets and shown by one- to four-week ahead horizon. 
#|  Note y-axis for mean WIS is cut-off to an upper limit of 2 for readability, 
#|  excluding ### evaluations of point predictions for cases (mean relative WIS ######)."

# How often did the ensemble beat any score from all model scores?
n_ensemble <- filter(model_eval, !is_hub) %>%
  group_by(target_variable) %>%
  summarise(n = n(),
            ensemble_beat = sum(ensemble_rel_wis < rel_wis, na.rm = TRUE),
            p_ensemble_beat = ensemble_beat / n * 100)
n_model_scores <- map(split(n_ensemble, n_ensemble$target_variable),
                       ~ .x %>% pull(n))
p_ensemble_beat <- map(split(n_ensemble, n_ensemble$target_variable),
                       ~ .x %>% pull(p_ensemble_beat) %>% round(0))

# PLOT: all models and ensemble performance by line over 1:4 week horizon
# average scores across locations for each model
h1_h4 <- model_eval %>%
  group_by(is_hub, model, horizon, target_variable) %>%
  summarise(is_hub = any(is_hub),
            n = n(),
            mean_wis = mean(rel_wis, na.rm = TRUE),
            sd_wis = sd(rel_wis, na.rm = TRUE),
            mean_cov50 = mean(cov_50, na.rm = TRUE),
            mean_cov95 = mean(cov_95, na.rm = TRUE))

# rel WIS per model by horizon
plot_model_wis <- h1_h4 %>%
  ggplot(aes(x = horizon, y = mean_wis, colour = model)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = 1), lty = 2, col = "black") +
  # highlight ensemble in red and add label to plot
  gghighlight(is_hub, calculate_per_facet = TRUE, use_direct_label = F) +
  labs(y = "Mean relative WIS", x = NULL) +
  scale_color_brewer(type = "qual", palette = 6) +
  facet_wrap(~ target_variable, scales = "fixed") +
  coord_cartesian(ylim = c(0, 2)) +
  theme(strip.background = element_blank(),
        legend.position = "none",
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) 

# Plot coverage by horizon
plot_model_coverage <- h1_h4 %>%  
  # show only 0.5 coverage level in plot
  mutate(expected = 0.5) %>%
  ggplot(aes(x = horizon, y = mean_cov50, colour = model)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = expected), lty = 2, colour = "black") +
  gghighlight(is_hub, calculate_per_facet = TRUE, use_direct_label = F) +
  scale_color_brewer(type = "qual", palette = 6) +
  labs(y = "50% coverage", x = "Weeks ahead horizon",
       fill = NULL, colour = NULL) +
  facet_wrap(~ target_variable, scales = "fixed") +
  theme(strip.background = element_blank(),
        strip.text = element_blank(),
        legend.position = "bottom") 

# Plot together
plot_model_wis +
  plot_model_coverage +
  plot_layout(nrow = 2)

```

The ensemble model performed well compared to both its component models and the baseline. In ranking all models' scores compared to the baseline, the ensemble performed better on relative WIS than `r p_ensemble_beat$Cases`% model scores when forecasting cases ($n=`r n_model_scores$Cases`$), and `r p_ensemble_beat$Deaths`% of scores for forecasts of incident deaths ($n=`r n_model_scores$Deaths`$). The ensemble outperformed the baseline model at the one-week ahead horizon for both cases and deaths (Figure 1). For horizons longer than one week, performance depended on the epidemiological target. The ensemble stopped outperforming the baseline at three to four weeks with respect to incidence cases. In contrast, the ensemble outperformed the baseline for deaths at all horizons considered (up to four weeks WIS), with no discernible deterioration in performance. 

We observed similar trends in performance when considering how well the ensemble was calibrated with respect to the observed data. At one week ahead the case ensemble was well calibrated (ca. 50% nominal coverage at the 50% level), but this did not hold with increasing forecast horizon. The death ensemble was well calibrated at the 95% level for all horizons and under confident for the 50% level, with only slow deterioration with increasing forecast horizon.

```{r figure-2-model-by-location}
#| fig.cap = "Performance of short-term forecasts across models and
#|  median ensemble (asterisk), by country, forecasting cases (top) and deaths
#|  (bottom) for two-week ahead forecasts, according to the relative weighted interval score. 
#|  Boxplots show interquartile ranges, with outliers as faded points, and the 
#|  ensemble model performance is marked by an asterisk.
#|  y-axis is cut-off to an upper bound of 4 for readability, excluding one data point
#|  (relative WIS 5.91, one model of cases in Netherlands)"

# How often did the ensemble beat the baseline across locations?
n_ensemble_loc <- model_eval %>%
  filter(is_hub) %>%
  group_by(target_variable) %>%
  summarise(n = n(), # n = 32 countries for each target
            p_ensemble_beat_loc = sum(rel_wis <= 1, na.rm = TRUE) / n * 100)
p_ensemble_beat_loc <- map(split(n_ensemble_loc, n_ensemble_loc$target_variable),
                       ~ .x %>% pull(p_ensemble_beat_loc) %>% round(0))

# best individual model performance
n_model_loc <- model_eval %>%
  filter(!is_hub) %>%
  group_by(model, target_variable) %>%
  summarise(n = n(), # n = 32 countries for each target
            p_model_beat_loc = sum(rel_wis <= 1, na.rm = TRUE) / n * 100,
            .groups = "drop") %>%
  filter(n == unique(n_ensemble_loc$n)) %>%
  arrange(desc(p_model_beat_loc)) %>%
  group_by(target_variable) %>%
  slice_max(p_model_beat_loc, n = 1)
p_model_beat_loc <- map(split(n_model_loc, n_model_loc$target_variable),
                       ~ .x %>% 
                         mutate(p_model_beat_loc = round(p_model_beat_loc, 0)))

# PLOT: All model and ensemble performance by boxplot by location at 2 wk horizon
model_eval %>%
  # Use 2 week horizon
  filter(horizon == 2) %>%
  mutate(rel_wis = ifelse(rel_wis == 0, NA, rel_wis)) %>%
  # plot structure: boxplot rel wis by location and horizon
  ggplot(aes(x = location_name, y = rel_wis,
             colour = target_variable,
             fill = target_variable)) +
  geom_boxplot(alpha = 0.8, 
               outlier.alpha = 0.2) +
  geom_hline(aes(yintercept = 1), lty = 2) +
  # overlay ensemble as extra point
  geom_point(aes(y = ensemble_rel_wis),
              size = 2, shape = "asterisk",
             colour = "grey 10",
             position = position_dodge(width = 0.8)) +
  # format
  ylim(c(0,4)) +
  labs(x = NULL, y = "Scaled relative WIS across models") +
  scale_fill_brewer(palette = "Set1") +
  scale_colour_brewer(palette = "Set1") +
  facet_wrap(~ target_variable, scales = "fixed", nrow = 2) +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 30, hjust = 1),
        strip.background = element_blank())
```

The ensemble also performed consistently well when forecasting across countries relative to individual models and the baseline (figure 2, figure SI1).  Compared to models that forecast across all 32 countries at the two-week horizon, the ensemble was more consistent in outperforming the baseline across countries compared to any single model forecasting deaths, and all but one model for case forecasts. Considering forecast targets across all 32 countries and over all four horizons (128 targets), the ensemble forecast outperformed the baseline for `r p_ensemble_beat_loc$Cases`% and `r p_ensemble_beat_loc$Deaths`% of all 128 targets when forecasting cases and deaths, respectively. Comparably, the best individual models forecasting for the same number of targets as the ensemble model outperformed the baseline for `r p_model_beat_loc$Cases$p_model_beat_loc`% ("`r p_model_beat_loc$Cases$model`" model) for cases and `r p_model_beat_loc$Deaths$p_model_beat_loc`% ("`r p_model_beat_loc$Deaths$model`" model) for death forecasts.

