---
title: "Full text report"
output:
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---
```{r set up, include=FALSE}
# Packages
library(here)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(ggplot2)
library(forcats)
library(patchwork)
library(gghighlight)
# Settings
this_doc <- "analysis/whole.Rmd"
here::i_am(this_doc)
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, 
                      message = FALSE, warning = FALSE,
                      eval.after = "fig.cap")
theme_set(theme_bw())
# Use trackdown for edits
# gdrive_path <- "trackdown/euro-hub-ensemble"
# trackdown::render_file(this_doc, gpath = gdrive_path)
# trackdown::download_file(this_doc, gpath = gdrive_path)
# trackdown::update_file(this_doc, gpath = gdrive_path)
```

## Predictive performance of multi-model ensemble forecasts of Covid-19 across European nations

_Order tbc;_ Katharine Sherratt, Hugo Gruson, _Any co-authors_, _Team authors_, _Advisory team authors_, _ECDC authors_, Johannes Bracher, Sebastian Funk


### Abstract

_Background_ Short-term forecasts of infectious disease burden can contribute to situational awareness and aid capacity planning. Based on best practice in other fields and recent insights in infectious disease epidemiology, the predictive performance of such forecasts is maximised if multiple models are combined into an ensemble. Here we report on the performance of ensembles created from over 40 models in predicting COVID-19 cases and deaths across 32 European countries.

_Methods_ We set up the European Covid-19 Forecast Hub in a publicly available repository and invited participation by groups globally. We solicited weekly forecasts as a combination of point forecasts and quantiles of the predictive distribution. Forecasts were time-stamped at submission and combined every week into an unweighted ensemble where each predictive quantile was calculated as either the median or mean of individual model predictive quantiles, as well as a suite of weighted ensembles where model weights calculated based on past predictive skill. The performance of the ensembles was compared to individual models and a baseline model of no change using pairwise comparison with the Weighted Interval Score (WIS).

_Results_ We found that any of our ensemble forecasts outperformed the baseline forecast, while using any form of median forecast was nearly always the most accurate choice of ensemble method. Ensemble forecasts for incident death counts had consistently better performance than forecasts of case counts. Performance remained relatively stable at longer horizons up to 4 weeks for death forecasts of any ensemble method, while performance worsened over time for case forecasts.

_Conclusions_ Our results support the use of an ensemble as a reliable way to make real-time forecasts during infectious disease epidemics. We recommend the use of median ensemble methods, and that policy relevant work that uses ensembles should place more confidence in forecasts of incident death than case counts, particularly at longer (more than 2 week) periods into the future.

### Background

Epidemiological forecasts make quantitative statements about a disease outcome in the near future. Multiple forecasts of Covid-19 outcomes have been critical to responding to the global outbreaks of Covid-19 since early 2020, for both policy makers and the general public [1]. At the most general level forecasting targets can include measures of disease incidence, prevalence, or severity, for example forecasting reported cases, hospitalisations, or deaths in some population over a specified time horizon. However, often studies present forecasts from a single model [2], with methods varying widely from structured mathematical to purely statistical models [3]. While these forecasts are individually useful, there are barriers to accessing and comparing between forecasts for the same target. These include variation in outcome definition, aggregation of time or space, or reporting of probability [2].

A "forecast hub" is a centralised effort to improve the transparency and utility of forecasts, by standardising and collating the work of many independent teams producing forecasts in the public domain [4]. A hub sets a commonly agreed structure for forecast targets, such as type of disease event, spatio-temporal units, or the set of quantiles of the probability distribution to include from probabilistic forecasts. Forecasters then adopt this structure to format existing model outputs, and at regular intervals contribute forecasts for centralised storage. This allows forecasts produced from diverse teams and methods to be visualised and quantitatively compared on a near-exact like to like basis, which can strengthen public and policy use of disease forecasts.

The underlying approach to creating a forecast hub was pioneered for forecasting influenza in the USA and quickly adapted for forecasts of short-term Covid-19 cases and deaths in US states and counties [5]. This has also been adopted to forecasts for subnational units in Germany and Poland [6]. Similar efforts exist at a national scale in Austria [7] and the UK [8] and for longer term scenario projections in the USA [9], while forecasts for Covid-19 mortality at a national level worldwide have also been passively collected and evaluated [10]. Somewhat comparably, weather forecasting has a long standing use of building ensembles of many models using diverse methods with standardised data and formatting [11,12].

The European Covid-19 Forecast Hub [13] is a project to collate short term forecasts of Covid-19 across 32 countries in the European region. The Hub is jointly run with the European Centre for Disease Control (ECDC), with the primary aim to provide reliable information about the short-term epidemiology of the COVID-19 pandemic to the research and policy communities and the general public. Second, the hub aims to create infrastructure for storing and analysing epidemiological forecasts made in real time by diverse research teams and methods across Europe. Third, the hub aims to maintain a community of infectious disease modellers underpinned by open science principles. We started formally collating and combining contributions to the European Forecast Hub in March 2021. Here, we investigate the predictive performance of forecasts contributed to the hub and their combination into weighted and unweighted ensembles.

### Methods
We developed infrastructure to host and analyse forecasts. We follow a similar structure and data format, and adapted processes and software provided by the US [14,15] and the German and Polish COVID-19 [16,17] forecast hubs. 

#### Forecast targets and standardisation 

We sought forecasts for reported weekly incident counts of cases and deaths from COVID-19 for each of 32 countries in the European region (including all countries of the European Union and European Free Trade Area, and separately the United Kingdom). Incidence was aggregated over the US epidemiological week definition of Sunday through Saturday. When predicting any single forecast target, teams could express probability by submitting predictions across a range of a pre-specified set of 23 quantiles in the probability distribution. At the first submission we also asked teams to add a single set of metadata briefly describing the forecasting team and methods. Teams could also submit a single point forecast without uncertainty. We maintain a full project specification for detailed submissions protocol [18]. 

With the complete dataset for the latest forecasting week available each Sunday, all forecasts were submitted to the hub on Monday. We used an automated validation programme to check that each new forecast conformed to standardised formatting. This included checking that predictions increased monotonically with each increasing quantile, that predictions were integer counts, as well as that forecasts conformed to consistent date and location definitions. This software was developed by the US forecast hub team using Python, manually adapted to the European hub requirements, and runs automatically using Github Actions. 

Each week we built an ensemble of all forecasts which updated each week after all forecasts had been validated. From the first week of forecasting from 8 March 2021, the ensemble method for summarising across forecasts was the mean average of all models at each predictive quantile for a given location, target, and horizon. From 26 July 2021 onwards the ensemble instead used a median average of all predictive quantiles, in order to mitigate the wide uncertainty produced by highly anomalous forecasts. We created an open and publicly accessible interface to the forecasts and ensemble, including an online visualization tool allowing viewers to see past data and interact with one or multiple forecasts for each country and target for up to four weeks' horizon (\#cite website). All forecast and meta data are freely available and held on Zoltar, a platform for hosting epidemiological forecasts (\#cite Zoltar link). 

#### Forecast evaluation 

We evaluated all previous forecasts against actual observed values for each model, stratified by the forecast horizon, location, and target. We calculated scores using the scoringutils R package [19] with observed data reported by Johns Hopkins University [20]. JHU data included a mix of national and aggregated subnational data for the 32 countries in the Hub. We removed any forecast surrounding (in the week of or after) a strongly anomalous data point. We focus here on measurements of calibration and the interval score. 

We explored coverage of probabilistic forecasts to find where 50% of observations matched predictions within the 50% forecast interval. The interval score accounts for both under and over prediction (the difference between an observed value and a single prediction) as well as overall sharpness of the forecast (width of the probability distribution). These three factors are added to create the interval score [21]. This means that the interval score is the same as the absolute error for single-value point forecasts. It is therefore possible to compare probabilistic interval scores with the absolute error when evaluating probabilistic and deterministic forecasts simultaneously. However, absolute scores are difficult to compare across different forecast targets, as scores measured on the scale of the data result in the dominance of locations with large epidemics. Meanwhile, using error scaled only relative to itself over-exposes small changes in the data with large percentage differences (although this can be useful in the context of a growing epidemic). 

To enable a level comparison among all forecast targets, we created a forecast to use as a baseline against which other forecasts could be evaluated. This model was designed as the simplest possible probabilistic forecasting model where each forecast repeats the latest week's data, with expanding uncertainty over time created by re-sampling the forecast at each horizon. We could then scale the interval and absolute error scores against those of the baseline predictions. For each model's scaled relative score, we took the mean score for each target (location, target variable, and time horizon), and then used a geometric mean of pairwise comparisons. This allowed like for like comparison across targets. This baseline and comparison model was developed by the US COVID-19 forecast hub and has been used similarly to compare COVID-19 forecasts [5]. 

#### Alternative ensemble methods 

We retrospectively explored alternative methods for ensembling forecasts for each target each week. We used both mean and median methods of averaging across forecasts, each used with two methods of weighting any individual predicted value. Unweighted ensembles took the average predicted value from all forecasts, giving equal contributions from all forecasts available for any given target. Weighted ensembles allocated weights to each forecast model based on past performance of an individual model before averaging. 

To create weights for component models, we measured past performance using the interval score. The interval score evaluates probabilistic forecasts by accounting for both calibration and sharpness of a forecast [21]. We excluded models which did not provide the total set of 23 prediction intervals from weighted ensembles. However, models varied in predicting any one or multiple targets combined from a choice of predicting case or death counts, for 32 countries, and at four forecast horizons (weeks ahead predictions). To account for this variation, we weighted the interval score based on comparing each model's score to every other model forecasting for the same target, creating a pairwise comparison tournament. We then took the geometric mean of these pairwise comparisons for each model. This resulted in a single score per model for each of two target counts, 32 locations, and four forecast horizons. Separately, at this point we also averaged these scores across forecast horizons. We took the weighted interval score of each model and scaled it against the performance of the baseline (flat) forecast, giving a measure of performance that accounted for each forecast's individual skill compared to all other equivalent forecasts and a simple baseline. We took the inverse of these scores to create weights on a scale of 0-1 and applied these to a model's forecast values at all quantile predictions for each model. We then averaged across these weighted values at each quantile. 

To evaluate the simple and weighted mean and median ensemble forecasts, we used the same measure of performance described above based on calculating the relative interval score scaled to a baseline, and explored coverage and the interval score.

### Results

```{r model-description}
# Get latest evaluation scores for models
source(here("code", "get-model-eval.R"))

# Note - All model descriptions include the hub ensemble model
# Evaluation period
n_weeks <- max(model_eval$n)
start_date <- format.Date(eval_date - weeks(n_weeks), "%d %B")
end_date <- format.Date(eval_date, "%d %B %Y")

# Number of forecasters
n_model <- length(unique(model_eval$model))
n_team <- length(unique(model_eval$team_name))

# Number of models with rel wis scores
n_model_wis <- filter(model_eval, !is.na(rel_wis)) %>%
  distinct(model) %>% nrow()
```

We scored all forecasts submitted weekly in real time over the `r n_weeks` week period from `r start_date` to `r end_date`. Each week, forecasts could predict incident cases and deaths, for 32 locations over the following 4 weeks, creating 256 possible forecast targets. We received `r n_model` unique forecasting models from `r n_team` separate forecasting teams. We added an ensemble model using all available forecasts for each possible target. 

We used this dataset to create `r nrow(model_eval)` forecasting scores, each summarising a unique combination of model, variable, country, and week ahead horizon. Not all teams forecast for all targets, nor across all quantiles of the predictive distribution for each target. `r n_model_wis` models provided sufficient quantiles that we could evaluate them using the relative weighted interval score (WIS).

```{r}
#| figure-model-by-horizon,
#| fig.cap = "Figure #: Performance of short-term forecasts, by relative
#|  interval score (relative to a baseline forecast, top) and coverage of the 0.5
#|  interval (the proportion of observed values that fell within the predicted
#|  50% range, bottom). The scores of each model (grey) and ensemble (red) are
#|  averaged across all forecast targets and shown by one- to four-week ahead horizon."

# How often did the ensemble beat any score from all model scores?
n_ensemble <- filter(model_eval, !is_hub) %>%
  group_by(target_variable) %>%
  summarise(n = n(),
            ensemble_beat = sum(ensemble_rel_wis < rel_wis, na.rm = TRUE),
            p_ensemble_beat = ensemble_beat / n * 100)
n_model_scores <- map(split(n_ensemble, n_ensemble$target_variable),
                       ~ .x %>% pull(n))
p_ensemble_beat <- map(split(n_ensemble, n_ensemble$target_variable),
                       ~ .x %>% pull(p_ensemble_beat) %>% round(0))

# PLOT: all models and ensemble performance by line over 1:4 week horizon
# average scores across locations for each model
h1_h4 <- model_eval %>%
  group_by(is_hub, model, horizon, target_variable) %>%
  summarise(is_hub = any(is_hub),
            n = n(),
            mean_wis = mean(rel_wis, na.rm = TRUE),
            sd_wis = sd(rel_wis, na.rm = TRUE),
            mean_cov50 = mean(cov_50, na.rm = TRUE),
            mean_cov95 = mean(cov_95, na.rm = TRUE))

# rel WIS per model by horizon
plot_model_wis <- h1_h4 %>%
  ggplot(aes(x = horizon, y = mean_wis, colour = model)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = 1), lty = 2, col = "black") +
  # highlight ensemble in red and add label to plot
  gghighlight(is_hub, calculate_per_facet = TRUE, use_direct_label = F) +
  labs(y = "Mean relative WIS", x = NULL) +
  scale_color_brewer(type = "qual", palette = 6) +
  facet_wrap(~ target_variable, scales = "fixed") +
  theme(strip.background = element_blank(),
        legend.position = "none",
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) 

# Plot coverage by horizon
plot_model_coverage <- h1_h4 %>%  
  # show only 0.5 coverage level in plot
  mutate(expected = 0.5) %>%
  ggplot(aes(x = horizon, y = mean_cov50, colour = model)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = expected), lty = 2, colour = "black") +
  gghighlight(is_hub, calculate_per_facet = TRUE, use_direct_label = F) +
  scale_color_brewer(type = "qual", palette = 6) +
  labs(y = "50% coverage", x = "Weeks ahead horizon",
       fill = NULL, colour = NULL) +
  facet_wrap(~ target_variable, scales = "fixed") +
  theme(strip.background = element_blank(),
        strip.text = element_blank(),
        legend.position = "bottom") 

# Plot together
plot_model_wis +
  plot_model_coverage +
  plot_layout(nrow = 2)

```

The ensemble model performed well compared to both the relative scores of each original forecast model scaled to the baseline, and the baseline model itself. In ranking all models' scores compared to the baseline, the ensemble performed better on relative WIS than `r p_ensemble_beat$Cases`% model scores when forecasting cases (n=`r n_model_scores$Cases`), and `r p_ensemble_beat$Deaths`% of scores for forecasts of incident deaths (n=`r n_model_scores$Deaths`). Compared to the baseline model alone, the ensemble outperformed at one week for both cases and deaths (figure #). For horizons longer than one-week ahead, performance depended on the epidemiological target. Using relative WIS, the ensemble stopped outperforming the baseline at three to four weeks for cases. In contrast, the ensemble outperformed the baseline for deaths at all horizons considered (up to four weeks WIS), with no discernible deterioration in performance. 

We observed similar trends in performance when considering how well the ensemble calibrated to uncertainty. At one week the case ensemble was well calibrated (ca. 50% nominal coverage), but this did not hold with increasing forecast horizon. The death ensemble was well calibrated at the 95% level for all horizons and under confident for the 50% level, with only slow deterioration with increasing forecast horizon.

```{r}
#| figure-model-by-location,
#| fig.cap = "Figure #: Performance of short-term forecasts across models and
#|  median ensemble (asterisk), by country, forecasting cases (top) and deaths
#|  (bottom) for two-week ahead forecasts. Performance measured by relative
#|  weighted interval score scaled against baseline. Boxplots show interquartile
#|  range, with outliers as faded points, and ensemble model performance marked
#|  by asterisk."

# How often did the ensemble beat the baseline across locations?
n_ensemble_loc <- model_eval %>%
  filter(is_hub) %>%
  group_by(target_variable) %>%
  summarise(n = n(), # n = 32 countries for each target
            p_ensemble_beat_loc = sum(rel_wis <= 1, na.rm = TRUE) / n * 100)
p_ensemble_beat_loc <- map(split(n_ensemble_loc, n_ensemble_loc$target_variable),
                       ~ .x %>% pull(p_ensemble_beat_loc) %>% round(0))

# PLOT: All model and ensemble performance by boxplot by location at 2 wk horizon
model_eval %>%
  # Use 1 week horizon
  filter(horizon == 2) %>%
  # plot structure: boxplot rel wis by location and horizon
  ggplot(aes(x = location_name, y = rel_wis,
             colour = target_variable,
             fill = target_variable)) +
  geom_boxplot(alpha = 0.8, 
               outlier.alpha = 0.2) +
  geom_hline(aes(yintercept = 1), lty = 2) +
  # overlay ensemble as extra point
  geom_point(aes(y = ensemble_rel_wis),
              size = 2, shape = "asterisk",
             colour = "grey 10",
             position = position_dodge(width = 0.8)) +
  # format
  ylim(c(0,4)) +
  labs(x = NULL, y = "Scaled relative WIS across models") +
  scale_fill_brewer(palette = "Set1") +
  scale_colour_brewer(palette = "Set1") +
  facet_wrap(~ target_variable, scales = "fixed", nrow = 2) +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 30, hjust = 1),
        strip.background = element_blank())
```

The ensemble also performed consistently well when forecasting across countries, relative to individual models and the baseline (figure #).  Compared to models that forecast across all 32 countries at the two week horizon, the ensemble was the most consistent in outperforming the baseline across countries compared to any single model forecasting deaths, and all but one model for case forecasts. Considering forecast targets across all 32 countries and over all four horizons (128 targets), the ensemble forecast outperformed the baseline for `r p_ensemble_beat_loc$Cases`% and `r p_ensemble_beat_loc$Deaths`% of all 128 targets when forecasting cases and deaths respectively.

```{r}
#| figure-alternative-ensembles,
#| fig.cap = paste0(
#| "Figure #: Performance of alternative ensemble methods at 2 week
#|  horizon, showing mean difference (triangle) in relative weighted
#|  interval score, with 48% and 96% probability (thick and thin line
#|  respectively). The difference in WIS is a comparison of
#|  scores from forecasts made from all possible combinations of methods,
#|  with a single element of ensemble method input changed.
#|  Reference categories are: 
#|  weighted v. unweighted (n=", 
#|  ensemble_change_n$Weighted, 
#|  "); median v. mean (n=", 
#|  ensemble_change_n$Median, 
#|  "); cutoff by WIS v. all models included (n=", 
#|  ensemble_change_n$`Cutoff rel. WIS < 1`, 
#|  "); relative WIS measures over 10 weeks of forecast history vs. all forecasts (n=",
#|  ensemble_change_n$`10 weeks history`, ")")
#
# OR-style plot showing relative impact of different ensemble methods on performance
# get eval, calculate differences between ensemble methods
source(here("code", "get-ensemble-eval.R"))
# plot differences summary
ggplot(ensemble_change_dtb, aes(x = change)) +
  # mean = triangle
  geom_point(aes(y = mean), pch = 2, size = 4) +
  # 48% interval = thick line
  geom_linerange(aes(ymin = low_48, ymax = high_48), lwd = 2) +
  # 96% interval = thin line
  geom_linerange(aes(ymin = low_96, ymax = high_96), lwd = 1) +
  # 0 change in wis = reference line
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x = NULL, y = "Change in relative WIS compared to reference") +
  coord_flip()

```

At the two-week ahead horizon, variations in ensemble methods made little difference to forecast scores. Ensembles that weighted forecasts showed no difference in performance to simple unweighted ensemble methods. Similarly, in choosing a method with which to weight forecasts, the choice of whether the use scores across all past forecasts, or scores evaluating only the most recent 10 weeks' forecast scores, made very little difference to the performance of the resulting ensemble (0 mean change in forecast score). The choice to exclude any forecast that scored worse than the baseline forecast ("cut off") affected the performance of the ensemble in both directions, overall slightly worsening performance (+`r ensemble_change_dtb[ensemble_change_dtb$change == "Cutoff rel. WIS < 1", "mean"]` relative WIS).  Using the median average was the only variation of ensemble method that typically improved performance, compared to using the mean average across any combination of ensemble method.

### Discussion


---


